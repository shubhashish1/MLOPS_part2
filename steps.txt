# MLOPS_part2

In this project we will be taking the wafe fault detection dataset and implement it in the MLOPS pipelines.

### Set up steps:

1. Create a conda environment : conda create -n wafer_fault_mlops_new python==3.9 -y
2. Activate the conda environment : conda activate wafer_fault_mlops
3. Download cookiecutter as it will be useful for MLOPS folder structure creation : pip install cookiecutter / (for python>=3.8) pip install cookiecutter-data-science
4. Now the next command is : ccds which will as k few basic questions which are : <br>
            a. Project name : wafer_main
            b. repo name : mlops_main
            c. module name : code_repo
            d. Author name : shubhashish1
            e. description : Wafer fault detection
            f. python version : 3.9
            g. select datset storage :
            h. select env. manager : virtualenv (1)
            i . select dependency file : requirements.txt (1)
            j. select pydata_package :
            k. Slect open source license : MIT(2)
            l. select docs :
            m. select include_code_scaffold:
   And it's done. We will be able to get a folder named mlops_main with the required folder structure.

   5. Now let's go to the mlops_main folder created for the code interpreter by : cd mlops_main
   6. Now let's open the vs for code creation in mlops_main folder by providing command : code
   7. Let's download the dataset from the git : https://github.com/iNeuron-Pvt-Ltd/wafer-dataset/tree/main
   8. Now we will do the git initialization with command : git init
   9. Then install dvc with command : pip install dvc
   10. Now initialize the DVC with command : dvc init
   11. Do the first commit and push the details to remote repository :
  
                 . git add .
                 . git commit -m "first commit"
                 . git branch -M main
                 . git remote add origin <url>/<USERNAME>/<REPONAME>.git
                 . git push -u origin main

12. We will not be doing all the coding in the main branch... hence we will move to the dev branch: git checkout -b dev
13. Let's remove the git tracking for Training and prediction files with command: git rm -r --cached Prediction_Batch_files/*.csv
14. And git commit -m "stop tracking Prediction_Batch_files"
13. Now let's add the training data files to dvc for the data tracking with command: dvc add Training_Batch_Files/ Prediction_Batch_files/
14. Add all the requirements to the requirements.txt file.
15. Now we will use our gdrive as the cloud location to store the different versions of data via dvc using command : pip install dvc[gdrive]
16. Now we will be adding a remote storage of gdrive for the dvc to do the data tracking and versioning.
17. Go to the google drive and then create a folder where you want to store the data for me it is DVCDataTracking -> mlops_live
18. Now go to the mlops_live folder and pick the id from it's url like from "https://drive.google.com/drive/u/0/folders/1x4dRhghnf_WLpaLbK5r5YyVhefaxIJ1y" url copy "1x4dRhghnf_WLpaLbK5r5YyVhefaxIJ1y" drive id.
19. Now to add this remote drive to dvc tracking type command : dvc remote add -d storage gdrive://1x4dRhghnf_WLpaLbK5r5YyVhefaxIJ1y the drive id.
20. Now commit the storage by command: git add .dvc/config && git commit -m "configure remote storage"
21. Let's create the gdrive-user-credentials.json with informations: 
22. We can add this in the GitHub repo settings -> secrets and variables -> Actions -> New Repository Secrets and add the details as: Name: GDRIVE_CREDENTIALS and Secret: 
23. We can add gdrive credentials secrets in GitHub repo secrets. This credentials can be found in command: .dvc >> temp >> gdrive-user-credentials.json
24. Once the desired gdrive is set for the dvc tracking we have to push the changes to this drive with command: dvc push
25. This thing can be done to any other storage as well like s3 bucket or container etc.
26. When ever we want to get the data changed in dvc we can use the command: dvc pull (This we can use every time we do the new coding or change anything in the existing code).
27. Now let's install all requirements by command: pip install -r requirements.txt
28. Now we have to collect the data and combine them into single file and also save it as process for the automated way. We have to go to the code_repo and create a file named pipeline_01_data_preparation.py
29. We can see the config and the datasource values in the CLI by command: python pipeline_01_data_preparation.py
30. If we want a different config value then we can do that by command: python pipeline_01_data_preparation.py --config
31. Now let's create config folder and a file inside that folder named: params.yaml
32. This params.yaml file will contain all the information we need.
33. params.yaml is a json file which will contain the config for all the stages of our project life cycle.
34. Now let's prepare the training_schema.json nd prediction_schema.json files containing the details(schema) for training and prediction data. 